import os
import shutil
import json
import pickle
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.backends.cuda 
from torch_geometric.data import InMemoryDataset, Data, DataLoader
from torch_geometric.nn import GATv2Conv, global_mean_pool
from torch.nn import TransformerEncoder, TransformerEncoderLayer
from rdkit import Chem
from rdkit import RDLogger
from lifelines.utils import concordance_index
import gc
import random

# ================= ğŸš€ é…ç½®åŒºåŸŸ =================
DATASET_NAME = 'davis'   # å¦‚æœè·‘ KIBAï¼Œæ”¹æˆ 'kiba' å³å¯
BATCH_SIZE = 256         # A100 æ˜¾å­˜å¤§ï¼Œ256 å¾ˆç¨³
EPOCHS_RESTART = 200     # ã€ä»å¤´è·‘ã€‘çš„æ€»è½®æ•°
EPOCHS_CONTINUE = 100    # ã€ç»­è·‘ã€‘è¿½åŠ çš„è½®æ•°
LR_INIT = 0.0005         # åˆå§‹å­¦ä¹ ç‡
LR_CONTINUE = 0.0001     # ç»­è·‘å­¦ä¹ ç‡ (å°ä¸€ç‚¹ï¼Œé˜²æ­¢éœ‡è¡)
ALPHA = 0.01             # å¯¹æ¯”æŸå¤±æƒé‡
SAVE_DIR = './checkpoints' 
DATA_ROOT = './data'
PROCESSED_DIR = f'./data/processed_{DATASET_NAME}'
# ==============================================

RDLogger.DisableLog('rdApp.*') 
if not os.path.exists(SAVE_DIR): os.makedirs(SAVE_DIR)

# A100 åŠ é€Ÿå¼€å…³
torch.set_float32_matmul_precision('high')
if torch.cuda.is_available():
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    torch.backends.cuda.enable_math_sdp(True)

# ---------------------------------------------------------
# 1. åŸºç¡€å·¥å…·å‡½æ•°
# ---------------------------------------------------------
def prepare_data():
    target_file = os.path.join(DATA_ROOT, DATASET_NAME, 'ligands_can.txt')
    if not os.path.exists(target_file):
        print(f"ğŸ“¦ ä¸‹è½½æ•°æ®ä¸­...")
        if os.path.exists("DeepDTA"): shutil.rmtree("DeepDTA")
        # å¦‚æœ GitHub è¿ä¸ä¸Šï¼Œè¯·æ‰‹åŠ¨ä¸Šä¼ æ•°æ®åˆ° ./data/davis
        os.system(f"git clone https://mirror.ghproxy.com/https://github.com/hkmztrk/DeepDTA.git")
        os.makedirs(DATA_ROOT, exist_ok=True)
        if os.path.exists(f"DeepDTA/data/{DATASET_NAME}"):
            shutil.move(f"DeepDTA/data/{DATASET_NAME}", DATA_ROOT)
        if os.path.exists("DeepDTA"): shutil.rmtree("DeepDTA")

def one_of_k_encoding(x, allowable_set):
    if x not in allowable_set: x = allowable_set[-1]
    return list(map(lambda s: x == s, allowable_set))

def atom_features(atom):
    return np.array(one_of_k_encoding(atom.GetSymbol(), ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na', 'Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H', 'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr', 'Pt', 'Hg', 'Pb', 'Unknown']) +
                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +
                    one_of_k_encoding(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +
                    one_of_k_encoding(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +
                    [atom.GetIsAromatic()], dtype=np.float32)

def smile_to_graph(smile):
    mol = Chem.MolFromSmiles(smile)
    if mol is None: return None
    features = [atom_features(atom) for atom in mol.GetAtoms()]
    edges = []
    for bond in mol.GetBonds():
        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])
        edges.append([bond.GetEndAtomIdx(), bond.GetBeginAtomIdx()])
    if len(edges) == 0: 
        edge_index = torch.empty((2, 0), dtype=torch.long)
    else:
        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()
    return torch.tensor(features, dtype=torch.float), edge_index

# ---------------------------------------------------------
# 2. Dataset å®šä¹‰
# ---------------------------------------------------------
class GeneralDataset(InMemoryDataset):
    def __init__(self, root, dataset_name, drugs, prots, y, transform=None, pre_transform=None):
        self.dataset_name = dataset_name
        self.drugs = drugs
        self.prots = prots
        self.y = y
        super().__init__(root, transform, pre_transform)
        try:
            self.data, self.slices = torch.load(self.processed_paths[0], weights_only=False)
        except:
            self.data, self.slices = torch.load(self.processed_paths[0])

    @property
    def processed_file_names(self): return [f'{self.dataset_name}_processed.pt']

    def process(self):
        data_list = []
        CHARPROTSET = { "A": 1, "C": 2, "B": 3, "E": 4, "D": 5, "G": 6, "F": 7, "I": 8, "H": 9, "K": 10, "M": 11, "L": 12, "O": 13, "N": 14, "Q": 15, "P": 16, "S": 17, "R": 18, "U": 19, "T": 20, "W": 21, "V": 22, "Y": 23, "X": 24, "Z": 25 }
        print(f"ğŸ”¨ æ­£åœ¨æ„å»ºå›¾æ•°æ® (åªéœ€ä¸€æ¬¡)...")
        for i in range(len(self.drugs)):
            x, edge_index = smile_to_graph(self.drugs[i])
            if x is None: continue
            target = [CHARPROTSET.get(c, 0) for c in self.prots[i]]
            if len(target) > 1000: target = target[:1000]
            else: target = target + [0]*(1000-len(target))
            data = Data(x=x, edge_index=edge_index, 
                        target=torch.tensor(target, dtype=torch.long).unsqueeze(0), 
                        y=torch.tensor([self.y[i]], dtype=torch.float))
            data_list.append(data)
        data, slices = self.collate(data_list)
        torch.save((data, slices), self.processed_paths[0])
        print(f"âœ… æ•°æ®å¤„ç†å®Œæˆ!")

# ---------------------------------------------------------
# 3. æ¨¡å‹å®šä¹‰
# ---------------------------------------------------------
class CL_DTA(nn.Module):
    def __init__(self):
        super().__init__()
        self.drug_conv1 = GATv2Conv(78, 128, heads=4, concat=False, dropout=0.1)
        self.drug_conv2 = GATv2Conv(128, 128, heads=4, concat=False, dropout=0.1)
        self.drug_conv3 = GATv2Conv(128, 128, heads=4, concat=False, dropout=0.1)
        encoder_layer = TransformerEncoderLayer(d_model=128, nhead=4, batch_first=True, dropout=0.1)
        self.prot_trans = TransformerEncoder(encoder_layer, num_layers=2)
        self.prot_embed = nn.Embedding(26, 128)
        self.prot_fc = nn.Linear(128, 128)
        self.regressor = nn.Sequential(nn.Linear(256, 512), nn.ReLU(), nn.Dropout(0.3), nn.Linear(512, 128), nn.ReLU(), nn.Linear(128, 1))
        self.drug_proj = nn.Sequential(nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 64))
        self.prot_proj = nn.Sequential(nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 64))

    def forward(self, data):
        x = F.elu(self.drug_conv1(data.x, data.edge_index))
        x = F.elu(self.drug_conv2(x, data.edge_index))
        x = self.drug_conv3(x, data.edge_index)
        d_emb = global_mean_pool(x, data.batch)
        p = self.prot_embed(data.target)
        p = self.prot_trans(p)
        p_emb = self.prot_fc(p.mean(dim=1))
        pred = self.regressor(torch.cat([d_emb, p_emb], dim=1))
        z_d = self.drug_proj(d_emb)
        z_p = self.prot_proj(p_emb)
        return pred, z_d, z_p

def contrastive_loss(z_i, z_j, temp=0.1):
    z_i = F.normalize(z_i, dim=1)
    z_j = F.normalize(z_j, dim=1)
    logits = torch.matmul(z_i, z_j.T) / temp
    labels = torch.arange(z_i.size(0)).to(z_i.device)
    return (F.cross_entropy(logits, labels) + F.cross_entropy(logits.T, labels)) / 2

# ---------------------------------------------------------
# 4. è®­ç»ƒé€»è¾‘ï¼šã€ä»å¤´å¼€å§‹ã€‘
# ---------------------------------------------------------
def run_restart():
    gc.collect()
    torch.cuda.empty_cache()
    prepare_data()
    
    print(f"\n{'='*40}")
    print(f"ğŸš€ {DATASET_NAME.upper()} å…¨æ–°è®­ç»ƒæ¨¡å¼ (Restart Mode)")
    print(f"ğŸ”¥ ç›®æ ‡: {EPOCHS_RESTART} Epochs | ç­–ç•¥: Logè½¬æ¢ + Z-Scoreæ ‡å‡†åŒ–")
    print(f"{'='*40}")
    
    # å¼ºåˆ¶æ¸…ç†æ—§ç¼“å­˜ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°çš„æ ‡å‡†åŒ–é€»è¾‘
    if os.path.exists(PROCESSED_DIR): shutil.rmtree(PROCESSED_DIR)
    
    data_path = f'{DATA_ROOT}/{DATASET_NAME}/'
    ligands = json.load(open(data_path + 'ligands_can.txt'))
    proteins = json.load(open(data_path + 'proteins.txt'))
    Y = pickle.load(open(data_path + 'Y', 'rb'), encoding='latin1')
    
    try:
        drug_keys = sorted(ligands.keys(), key=lambda x: int(x))
        prot_keys = sorted(proteins.keys(), key=lambda x: int(x))
    except:
        drug_keys = sorted(ligands.keys())
        prot_keys = sorted(proteins.keys())

    drugs_list = [ligands[k] for k in drug_keys]
    prots_list = [proteins[k] for k in prot_keys]
    
    drugs, prots, affinities = [], [], []
    raw_y_sample = []
    
    for i in range(len(drugs_list)):
        for j in range(len(prots_list)):
            try:
                val = Y[i][j]
                if not np.isnan(val):
                    drugs.append(drugs_list[i])
                    prots.append(prots_list[j])
                    affinities.append(val)
                    if len(raw_y_sample) < 100: raw_y_sample.append(val)
            except: continue
            
    affinities = np.array(affinities)
    
    # 1. Log è½¬æ¢
    if np.mean(raw_y_sample) > 100: 
        print(f"âš ï¸ æ£€æµ‹åˆ°æ•°å€¼è¾ƒå¤§ï¼Œæ‰§è¡Œ -Log10 è½¬æ¢...")
        affinities = -np.log10(affinities / 1e9)
        
    # 2. Z-Score æ ‡å‡†åŒ– (å¹¶ä¿å­˜å‚æ•°ï¼)
    Y_mean = np.mean(affinities)
    Y_std = np.std(affinities)
    affinities_norm = (affinities - Y_mean) / Y_std
    
    print(f"ğŸ“Š ç»Ÿè®¡å¹¶ä¿å­˜: Mean={Y_mean:.4f}, Std={Y_std:.4f}")
    with open(f'{SAVE_DIR}/{DATASET_NAME}_stats.pkl', 'wb') as f:
        pickle.dump({'mean': Y_mean, 'std': Y_std}, f)
    
    dataset = GeneralDataset(root=PROCESSED_DIR, dataset_name=DATASET_NAME, 
                             drugs=drugs, prots=prots, y=affinities_norm)
    
    train_size = int(0.8 * len(dataset))
    train_set, test_set = torch.utils.data.random_split(dataset, [train_size, len(dataset)-train_size])
    
    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=16, pin_memory=True)
    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=16, pin_memory=True)
    
    device = torch.device('cuda')
    model = CL_DTA().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=LR_INIT)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)
    mse_crit = nn.MSELoss()
    
    best_ci = -1
    
    print("\nâš¡ï¸ è®­ç»ƒå¼€å§‹...")
    for epoch in range(EPOCHS_RESTART):
        model.train()
        total_loss = 0
        for batch in train_loader:
            batch = batch.to(device, non_blocking=True)
            optimizer.zero_grad()
            pred, z_d, z_p = model(batch)
            loss = mse_crit(pred.flatten(), batch.y) + ALPHA * contrastive_loss(z_d, z_p)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            
        model.eval()
        preds, targets = [], []
        with torch.no_grad():
            for batch in test_loader:
                batch = batch.to(device, non_blocking=True)
                p, _, _ = model(batch)
                p_real = p * Y_std + Y_mean
                y_real = batch.y * Y_std + Y_mean
                preds.extend(p_real.cpu().numpy().flatten())
                targets.extend(y_real.cpu().numpy().flatten())
        
        ci = concordance_index(targets, preds)
        mse = np.mean((np.array(preds) - np.array(targets))**2)
        scheduler.step(mse)
        current_lr = optimizer.param_groups[0]['lr']
        
        print(f"Epoch {epoch+1:03d} | Loss: {total_loss/len(train_loader):.4f} | MSE: {mse:.4f} | CI: {ci:.4f} | LR: {current_lr:.1e}")
        
        if ci > best_ci:
            best_ci = ci
            torch.save(model.state_dict(), f'{SAVE_DIR}/{DATASET_NAME}_optimal.pth')
            
    print(f"ğŸ† è®­ç»ƒç»“æŸ! æœ€ä½³ CI: {best_ci:.4f}")

# ---------------------------------------------------------
# 5. è®­ç»ƒé€»è¾‘ï¼šã€ç»­è·‘æ¨¡å¼ã€‘
# ---------------------------------------------------------
def run_continue():
    gc.collect()
    torch.cuda.empty_cache()
    
    print(f"\n{'='*40}")
    print(f"ğŸ”„ {DATASET_NAME.upper()} ç»­è·‘æ¨¡å¼ (Continue Mode)")
    print(f"ğŸ”¥ ç›®æ ‡: è¿½åŠ  {EPOCHS_CONTINUE} Epochs | ç­–ç•¥: åŠ è½½æ—§æƒé‡ + å¾®è°ƒ")
    print(f"{'='*40}")
    
    # 1. æ£€æŸ¥å¿…è¦æ–‡ä»¶
    model_path = f'{SAVE_DIR}/{DATASET_NAME}_optimal.pth'
    stats_path = f'{SAVE_DIR}/{DATASET_NAME}_stats.pkl'
    if not os.path.exists(model_path) or not os.path.exists(stats_path):
        print(f"âŒ ä¸¥é‡é”™è¯¯: ç»­è·‘æ‰€éœ€æ–‡ä»¶ä¸¢å¤±ï¼è½¬ä¸ºã€é‡æ–°è®­ç»ƒã€‘...")
        run_restart()
        return

    # 2. åŠ è½½ç»Ÿè®¡å‚æ•° (ä¿è¯æ ‡å‡†åŒ–ä¸€è‡´)
    with open(stats_path, 'rb') as f:
        stats = pickle.load(f)
        Y_mean = stats['mean']
        Y_std = stats['std']
    print(f"ğŸ“Š å·²åŠ è½½å†å²ç»Ÿè®¡: Mean={Y_mean:.4f}, Std={Y_std:.4f}")

    # 3. å‡†å¤‡æ•°æ® (é‡ç”¨ processed ç¼“å­˜)
    # å¿…é¡»é‡æ–°è¯»å– keys åˆ—è¡¨æ¥åˆå§‹åŒ– Datasetï¼Œä½† Data å¯¹è±¡ä¼šä»ç¡¬ç›˜ç¼“å­˜åŠ è½½
    data_path = f'{DATA_ROOT}/{DATASET_NAME}/'
    ligands = json.load(open(data_path + 'ligands_can.txt'))
    proteins = json.load(open(data_path + 'proteins.txt'))
    
    try:
        drug_keys = sorted(ligands.keys(), key=lambda x: int(x))
        prot_keys = sorted(proteins.keys(), key=lambda x: int(x))
    except:
        drug_keys = sorted(ligands.keys())
        prot_keys = sorted(proteins.keys())
    drugs_list = [ligands[k] for k in drug_keys]
    prots_list = [proteins[k] for k in prot_keys]
    
    # è¿™é‡Œçš„ y=[0]*len ä»…ä»…æ˜¯ä¸ºäº†å ä½ï¼ŒçœŸæ­£çš„æ•°æ®åœ¨ processed.pt é‡Œ
    dataset = GeneralDataset(root=PROCESSED_DIR, dataset_name=DATASET_NAME, 
                             drugs=drugs_list, prots=prots_list, y=[0]*len(drugs_list))
    
    train_size = int(0.8 * len(dataset))
    train_set, test_set = torch.utils.data.random_split(dataset, [train_size, len(dataset)-train_size])
    
    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=16, pin_memory=True)
    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=16, pin_memory=True)

    # 4. åŠ è½½æ¨¡å‹æƒé‡
    device = torch.device('cuda')
    model = CL_DTA().to(device)
    print(f"ğŸ“‚ åŠ è½½æƒé‡: {model_path}")
    model.load_state_dict(torch.load(model_path))
    
    # 5. é…ç½®ä¼˜åŒ–å™¨ (å°å­¦ä¹ ç‡)
    optimizer = torch.optim.Adam(model.parameters(), lr=LR_CONTINUE)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)
    mse_crit = nn.MSELoss()

    # 6. æ£€æµ‹å½“å‰æ°´å¹³
    model.eval()
    temp_preds, temp_targets = [], []
    with torch.no_grad():
        for batch in test_loader:
            batch = batch.to(device)
            p, _, _ = model(batch)
            p_real = p * Y_std + Y_mean
            y_real = batch.y * Y_std + Y_mean
            temp_preds.extend(p_real.cpu().numpy().flatten())
            temp_targets.extend(y_real.cpu().numpy().flatten())
    best_ci = concordance_index(temp_targets, temp_preds)
    print(f"ğŸ å½“å‰æ¨¡å‹èµ·ç‚¹ CI: {best_ci:.4f}")

    # 7. ç»­è·‘å¾ªç¯
    print("\nâš¡ï¸ ç»§ç»­å¾®è°ƒ...")
    for epoch in range(EPOCHS_CONTINUE):
        model.train()
        total_loss = 0
        for batch in train_loader:
            batch = batch.to(device, non_blocking=True)
            optimizer.zero_grad()
            pred, z_d, z_p = model(batch)
            loss = mse_crit(pred.flatten(), batch.y) + ALPHA * contrastive_loss(z_d, z_p)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            
        model.eval()
        preds, targets = [], []
        with torch.no_grad():
            for batch in test_loader:
                batch = batch.to(device, non_blocking=True)
                p, _, _ = model(batch)
                p_real = p * Y_std + Y_mean
                y_real = batch.y * Y_std + Y_mean
                preds.extend(p_real.cpu().numpy().flatten())
                targets.extend(y_real.cpu().numpy().flatten())
        
        ci = concordance_index(targets, preds)
        mse = np.mean((np.array(preds) - np.array(targets))**2)
        scheduler.step(mse)
        current_lr = optimizer.param_groups[0]['lr']
        
        # æ˜¾ç¤ºä¸º "200 + N" è½®
        print(f"Epoch {EPOCHS_RESTART + epoch + 1:03d} | Loss: {total_loss/len(train_loader):.4f} | MSE: {mse:.4f} | CI: {ci:.4f} | LR: {current_lr:.1e}")
        
        if ci > best_ci:
            best_ci = ci
            print(f"   ğŸ”¥ æ–°çºªå½•! æ›´æ–°æ¨¡å‹ (CI: {best_ci:.4f})")
            torch.save(model.state_dict(), model_path)

    print(f"ğŸ† ç»­è·‘ç»“æŸ! æœ€ç»ˆæœ€ä½³ CI: {best_ci:.4f}")

# ================= ğŸš€ ä¸»ç¨‹åºå…¥å£ =================
if __name__ == "__main__":
    # 1. å›ºå®šéšæœºç§å­ (éå¸¸å…³é”®ï¼å¦åˆ™ç»­è·‘çš„æ•°æ®åˆ’åˆ†ä¼šå˜)
    SEED = 42
    random.seed(SEED)
    np.random.seed(SEED)
    torch.manual_seed(SEED)
    torch.cuda.manual_seed_all(SEED)
    
    # 2. è‡ªåŠ¨åˆ¤æ–­é€»è¾‘
    checkpoint_file = f'{SAVE_DIR}/{DATASET_NAME}_optimal.pth'
    
    if os.path.exists(checkpoint_file):
        print(f"ğŸ“‚ æ£€æµ‹åˆ°å·²æœ‰æ¨¡å‹: {checkpoint_file}")
        # å¦‚æœä½ æƒ³å¼ºåˆ¶é‡è·‘ï¼Œå¯ä»¥æ‰‹åŠ¨åˆ é™¤ checkpoints æ–‡ä»¶å¤¹ï¼Œæˆ–è€…æ³¨é‡Šæ‰è¿™è¡Œ
        run_continue()
    else:
        print(f"ğŸ“‚ æœªæ£€æµ‹åˆ°æ¨¡å‹ï¼Œå‡†å¤‡ä»é›¶å¼€å§‹...")
        run_restart()
